{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# Set project root\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_curve, roc_auc_score, precision_recall_curve, auc, precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "#--- Get absolute path to the utils folder---\n",
    "from pathlib import Path\n",
    "utils_path = Path.cwd() / \"utils\"\n",
    "sys.path.append(str(utils_path))\n",
    "\n",
    "backtesting_path = Path.cwd() / \"backtesting\"\n",
    "sys.path.append(str(backtesting_path))\n",
    "\n",
    "from notebooks.utils.config_utils import load_config\n",
    "from notebooks.utils.data_utils import load_dataset, preprocess_data\n",
    "from notebooks.utils.model_utils import initialise_model, grid_search, get_feature_importance, optimise_threshold\n",
    "from notebooks.utils.plotting_utils import plot_correlation, plot_calibration_curve, plot_scatter, plot_roc_and_prc, plot_classification_report, plot_point_biserial_correlation, plot_dataset_split\n",
    "from notebooks.utils.pdf_utils import create_markdown_report, update_markdown_with_model_details, convert_markdown_to_html, save_pdf_from_html\n",
    "\n",
    "from notebooks.backtesting.backtester import Backtester\n",
    "from notebooks.dashboard.dashboard import ModelDashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test load\n",
    "config = load_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset(config)\n",
    "df_selected, selected_features, constructed_features, target_variable = preprocess_data(df, config)\n",
    "\n",
    "df_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot correlation heatmap (for selected features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_correlation_image_path = plot_correlation(df_selected, selected_features, constructed_features, target_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Point-Biserial Correlation and P-values\n",
    "\n",
    "- **Point-biserial correlation**: Measures the strength and correlation between a binary variable (target) and continuous variables (each feature) (higher value better)\n",
    "- **P-value**: Indicates if the correlation is statistically significant (lower value better)\n",
    "- **Combined Score**: A reflective measure of both Point-biserial and P-val."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_biserial_correlation_image_path = plot_point_biserial_correlation(df_selected, selected_features, constructed_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Pipeline\n",
    "\n",
    "- **Feature Selection**: \n",
    "    - Trains only on selected and constructed features\n",
    "- **Data Splitting**:\n",
    "    - Exclude the last 500 rows for testing\n",
    "    - Split remaining data into 80% train and 20% validation sets\n",
    "- **Model Training Loop**:\n",
    "    1) Initialise each model from config yaml\n",
    "    2) Apply MinMax scaling... *only for models that require scaling*\n",
    "    3) Perform grid search for hyperparameter tuning (if specified) and Train Model\n",
    "    4) Apply calibration (optional), predict on validation set and display feature importance\n",
    "    5) Optimise precision-recall threshold\n",
    "    6) Evaluate model on validation set using optimised threshold\n",
    "    7) Plot ROC and Precision-Recall graphs\n",
    "    8) Predict on the test set (last 500 rows) and evaluate\n",
    "    9) Save the trained model\n",
    "    10) Save predictions (for backtesting).\n",
    "    11) Plot Scatter Graph.\n",
    "    12) Backtesting called\n",
    "    13) PDF Report generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classification_pipeline(config, df, selected_features, constructed_features, target_variable, show_output=False, generate_pdf=False, track_num=1): \n",
    "    models_to_train = config['model']['classification']['models']\n",
    "    apply_calibration = config['apply_calibration']\n",
    "\n",
    "    # Initialize the dashboard and get a pipeline run ID\n",
    "    dashboard = ModelDashboard(track_num=track_num)\n",
    "\n",
    "    model_results_dict = {}\n",
    "    \n",
    "    # Step 1: Create the Markdown for the report\n",
    "    if generate_pdf:\n",
    "        markdown_content = create_markdown_report(config, feature_correlation_image_path, point_biserial_correlation_image_path, target_variable, selected_features, constructed_features, models_to_train, apply_calibration)   \n",
    "    #Only train on selected and constructed features\n",
    "    X = df[selected_features + constructed_features]\n",
    "    y = df[target_variable]\n",
    "\n",
    "    #Get num of 1s and 0s\n",
    "    value_counts=y.value_counts()\n",
    "    num_ones=value_counts.get(1, 0)\n",
    "    num_zeros=value_counts.get(0, 0)\n",
    "    total=value_counts.sum()\n",
    "    implied_1_plus_betting_odds = round(1/(num_ones/total), 4)\n",
    "\n",
    "    print(\"--- DATASET STATS ---\")\n",
    "    print(f\"Number of 1's: {num_ones}\")\n",
    "    print(f\"Number of 0's: {num_zeros}\")\n",
    "    print(f\"Total: {total}\")\n",
    "    print(f\"Implied 1+ betting odds = {implied_1_plus_betting_odds}\")\n",
    "    print(\"---------------------\")\n",
    "\n",
    "    #Split data to exclude the last 500 rows for testing\n",
    "    train_data = df.iloc[:-500]\n",
    "    test_data = df.iloc[-500:]\n",
    "\n",
    "    # Split data -> train & validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        train_data[selected_features + constructed_features],\n",
    "        train_data[target_variable],\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=train_data[target_variable]\n",
    "    )\n",
    "\n",
    "    # Create X and y for testset\n",
    "    X_test= test_data[selected_features+constructed_features]\n",
    "    y_test =test_data[target_variable]\n",
    "\n",
    "    # MinMax Scaling for models that require scaling...\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled =scaler.transform(X_val)\n",
    "    X_test_scaled =scaler.transform(X_test)\n",
    "\n",
    "    #Plots the train/val/test split of the dataset\n",
    "    plot_dataset_split(train_data, X_val, y_val, test_data, target_variable, show_output, track_num=track_num)\n",
    "\n",
    "    # --- MODEL TRAINING LOOP ---\n",
    "    # Train and evaluate each model from the config\n",
    "    for model_name in models_to_train:\n",
    "        print(f\"\\n-> Training {model_name}...\")\n",
    "        if model_name not in model_results_dict:\n",
    "            model_results_dict[model_name] = {}\n",
    "    \n",
    "        # Get the hyperparameters for the model\n",
    "        hyperparameters = config[\"model\"][\"classification\"][\"hyperparameters\"].get(model_name, {})\n",
    "        do_grid_search=config[\"model\"][\"classification\"].get(\"grid_search\", False)\n",
    "\n",
    "        # --- STEP 1: Initialise the model\n",
    "        model = initialise_model(model_name, hyperparameters)\n",
    "\n",
    "        # --- STEP 2: Select Scaled or Unscaled Data ---\n",
    "        if model_name in [\"logistic_regression\", \"svc\", \"xgboost\"]:\n",
    "            X_train, X_val, X_test = X_train_scaled, X_val_scaled, X_test_scaled\n",
    "        \n",
    "        # --- STEP 3: Grid Search (optional) ---\n",
    "        if do_grid_search:\n",
    "            optimal_model = grid_search(model_name, model, X_train, y_train, show_output)\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "            optimal_model = model\n",
    "\n",
    "        # --- STEP 4: Apply Calibration (optional) and predict on the validation set ---\n",
    "        if apply_calibration:\n",
    "            if show_output:\n",
    "                print(f\"Applying probability calibration to {model_name}...\")\n",
    "            \n",
    "            cv_param = 2\n",
    "            calibrated_model = CalibratedClassifierCV(optimal_model, method='sigmoid', cv=cv_param)\n",
    "            calibrated_model.fit(X_val, y_val)\n",
    "            optimal_model=calibrated_model\n",
    "\n",
    "            y_pred_val = optimal_model.predict_proba(X_val)[:, 1]\n",
    "            plot_calibration_curve(y_val, y_pred_val, model_name, show_output)\n",
    "        else:\n",
    "            y_pred_val = optimal_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        #Display Feature Importance\n",
    "        feature_importances = get_feature_importance(optimal_model, model_name, selected_features, constructed_features)\n",
    "        if show_output:\n",
    "            print(\"\\n### Top 8 and Bottom 5 Feature Importance ###\")\n",
    "            print(feature_importances)\n",
    "\n",
    "        # --- STEP 5: Precision-Recall Threshold Optimisation ---\n",
    "        best_threshold, optimal_threshold = optimise_threshold(y_pred_val, y_val, show_output=show_output, min_recall=0.05)\n",
    "\n",
    "        # optimal_threshold=0\n",
    "        # --- STEP 6: Model Evaluation ---\n",
    "        y_pred_threshold =(y_pred_val >= optimal_threshold).astype(int)\n",
    "        if show_output:\n",
    "            print(f\"\\n### Classification Report (threshold={optimal_threshold}):\\n\") \n",
    "            print(classification_report(y_val, y_pred_threshold))\n",
    "        classification_report_str_1 = classification_report(y_val, y_pred_threshold, output_dict=False)\n",
    "\n",
    "        # --- STEP 7: Plot ROC and Precision-Recall curves ---\n",
    "        fpr,tpr,_ =roc_curve(y_val, y_pred_val)\n",
    "        roc_auc =roc_auc_score(y_val, y_pred_val)\n",
    "        precision,recall,_ =precision_recall_curve(y_val, y_pred_val)\n",
    "        pr_auc=auc(recall, precision)\n",
    "\n",
    "        # --- STEP 8: Test on test set (last 500 rows) ---\n",
    "        # Predict on the final test data (last 500 rows)\n",
    "        y_probs_final =optimal_model.predict_proba(X_test)[:, 1]\n",
    "        y_pred_final=(y_probs_final >=optimal_threshold).astype(int)\n",
    "\n",
    "        #Evaluate on the last 500 rows (final simulation)\n",
    "        if show_output:\n",
    "            print(\"\\n### Prediction on last 500 rows: ###\")\n",
    "            print(classification_report(y_test, y_pred_final))\n",
    "        classification_report_str_2 = classification_report(y_test, y_pred_final, output_dict=False)\n",
    "\n",
    "        classification_report_image_path = plot_classification_report(optimal_model, X_val,y_val,X_test,y_test, model_name, show_output)\n",
    "        roc_prc_image_path = plot_roc_and_prc(fpr, tpr, roc_auc, precision, recall, pr_auc,model_name,show_output)\n",
    "\n",
    "        # --- STEP 9: Save Model ---\n",
    "        model_dir = \"../models\" \n",
    "        if not os.path.exists(model_dir): # Ensure the directory exists\n",
    "            os.makedirs(model_dir)\n",
    "        # Save the model\n",
    "        joblib.dump(optimal_model, os.path.join(model_dir, f\"{model_name.replace(' ', '_').lower()}_model.pkl\"))\n",
    "        if show_output:\n",
    "            print(f\"{model_name} model saved.\")\n",
    "\n",
    "        prediction_file = f\"../data/predictions/{model_name.replace(' ', '_').lower()}_predictions.csv\"\n",
    "        # --- STEP 10: Save Predictions ---\n",
    "        results_df = pd.DataFrame({\n",
    "            'kaggle_id': test_data['id_odsp'],\n",
    "            'model_predicted_binary': y_pred_final,\n",
    "            'actual_result': y_test\n",
    "        })\n",
    "        results_df.to_csv(prediction_file, index=False)\n",
    "        if show_output:\n",
    "            print(f\"Predictions saved for {model_name}.\")\n",
    "\n",
    "        # --- STEP 11: Plot Scatter Graph ---\n",
    "        scatter_image_path = plot_scatter(y_probs_final, y_test, model_name, show_output, optimal_threshold=optimal_threshold)\n",
    "\n",
    "        # --- STEP 12: Backtesting called ---\n",
    "        if show_output:\n",
    "            print(f\"\\n-> Running Backtest for {model_name}...\")\n",
    "        odds_file = config[\"paths\"][\"total_corner_odds\"]\n",
    "        backtester = Backtester(config, odds_file=odds_file, model_file=prediction_file, model_type=\"classification\", target_mean=implied_1_plus_betting_odds)\n",
    "        backtesting_image_path, backtesting_results_str_list, backtesting_results_dict = backtester.run(show_output)\n",
    "\n",
    "        # Calculate eval metrics for validation and test set\n",
    "        model_results_dict[model_name]['precision_val'] = round(precision_score(y_val, y_pred_threshold), 3)\n",
    "        model_results_dict[model_name]['recall_val'] = round(recall_score(y_val, y_pred_threshold), 3)\n",
    "        model_results_dict[model_name]['f1_score_val'] = round(f1_score(y_val, y_pred_threshold), 3)\n",
    "        model_results_dict[model_name]['accuracy_val'] = round(accuracy_score(y_val, y_pred_threshold), 3)\n",
    "\n",
    "        model_results_dict[model_name]['precision_test'] = round(precision_score(y_test, y_pred_final), 3)\n",
    "        model_results_dict[model_name]['recall_test'] = round(recall_score(y_test, y_pred_final), 3)\n",
    "        model_results_dict[model_name]['f1_score_test'] = round(f1_score(y_test, y_pred_final), 3)\n",
    "        model_results_dict[model_name]['accuracy_test'] = round(accuracy_score(y_test, y_pred_final), 3)\n",
    "\n",
    "        # Add backtesting results to model_results_dict\n",
    "        model_results_dict[model_name].update(backtesting_results_dict)\n",
    "\n",
    "        # --- STEP 13: PDF Report generated. ---\n",
    "        if generate_pdf:\n",
    "            #Finally, update markdown with generated outputs...\n",
    "            markdown_content = update_markdown_with_model_details(\n",
    "                markdown_content,\n",
    "                model_name,\n",
    "                feature_importances,\n",
    "                best_threshold,\n",
    "                classification_report_str_1,\n",
    "                classification_report_str_2,\n",
    "                classification_report_image_path,\n",
    "                roc_prc_image_path,\n",
    "                scatter_image_path,\n",
    "                backtesting_results_str_list,\n",
    "                backtesting_image_path\n",
    "            )\n",
    "\n",
    "    #Now all models have been trained... add each results to dashboard\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M\") #get timestamped before loop so they all are the same\n",
    "    for model_name, model_results in model_results_dict.items():\n",
    "        dashboard.add_model_results(\n",
    "            timestamp, \n",
    "            model_name,\n",
    "            model_results,\n",
    "            selected_features,\n",
    "            constructed_features,\n",
    "            apply_calibration\n",
    "        )\n",
    "\n",
    "    if generate_pdf:\n",
    "        now = datetime.now()\n",
    "        date_time_str = now.strftime(\"%Y-%m-%d, %H:%M\")\n",
    "        # Convert Markdown to html and save as pdf to reports/model_reports/\n",
    "        html_content = convert_markdown_to_html(markdown_content)\n",
    "        save_pdf_from_html(html_content, f'../reports/model_reports/{date_time_str}_model_report.pdf')\n",
    "        print(\"ðŸ“„ Saved PDF Report\")\n",
    "    \n",
    "    print(\"âœ… Finished Running Pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_classification_pipeline(config, df_selected, selected_features, constructed_features, target_variable=target_variable[0], show_output=False, generate_pdf=True, track_num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
