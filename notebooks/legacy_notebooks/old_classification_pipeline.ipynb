{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import sys\n",
    "# Set project root\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import seaborn as sns\n",
    "\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, make_scorer, roc_curve, roc_auc_score, precision_recall_curve, auc, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "\n",
    "import base64\n",
    "from weasyprint import HTML\n",
    "import markdown\n",
    "from datetime import datetime\n",
    "\n",
    "#--- Get absolute path to the utils folder---\n",
    "from pathlib import Path\n",
    "utils_path = Path.cwd() / \"utils\"\n",
    "sys.path.append(str(utils_path))\n",
    "\n",
    "from notebooks.utils.pdf_utils import (\n",
    "    create_markdown_report,\n",
    "    update_markdown_with_model_details,\n",
    "    save_pdf_from_html,\n",
    "    convert_markdown_to_html\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get config file\n",
    "config_path = os.path.join(project_root, \"config.yaml\")\n",
    "\n",
    "def load_config():\n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f\"ERROR: config.yaml not found at {config_path}\")\n",
    "    else:\n",
    "        with open(config_path, \"r\") as file:\n",
    "            config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "# Test load\n",
    "config = load_config()\n",
    "print(\"Config loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import `aggregated_full.csv` Dataset and Select features from config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "READ_FOLDER_PATH = '../data/processed/'\n",
    "\n",
    "df = pd.read_csv(READ_FOLDER_PATH+'aggregated_full.csv')\n",
    "\n",
    "print(df.shape[0])\n",
    "pd.set_option(\"display.max_colwidth\", None) \n",
    "\n",
    "context_features = config.get(\"features\",{}).get(\"context_features\",[])\n",
    "selected_features = config.get(\"features\",{}).get(\"selected_features\",[])\n",
    "constructed_features = config.get(\"features\",{}).get(\"constructed_features\",[])\n",
    "target_variables =config.get(\"features\",{}).get(\"target_variables\",[])\n",
    "\n",
    "if constructed_features is None:\n",
    "    constructed_features = []\n",
    "\n",
    "selected_columns = context_features + selected_features + constructed_features + target_variables\n",
    "df = df[selected_columns]\n",
    "\n",
    "#Drop NaN and inf rows:\n",
    "df_selected = df.copy()\n",
    "df_selected.loc[:, :] = df.replace([np.inf, -np.inf], np.nan)\n",
    "df_selected = df_selected.dropna()\n",
    "df_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot correlation heatmap (for selected features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corelation matrix:\n",
    "correlation_matrix = df_selected[selected_features + constructed_features + target_variables].corr()\n",
    "correlation_with_target = correlation_matrix[target_variables].drop(target_variables, axis=0)\n",
    "\n",
    "# Plot correlation heatmap:\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_with_target, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.xlabel(\"Target Variables\")\n",
    "plt.ylabel(\"Constructed Features\")\n",
    "plt.title(\"Correlation between Constructed Features and Target Variables\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save graph as an image\n",
    "feature_correlation_image_path = f\"../reports/images/feature_correlation.png\"\n",
    "plt.savefig(feature_correlation_image_path)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Funcs for Model Training:\n",
    "\n",
    "- `Initialise_model`\n",
    "    - Initialises classifier model depending on input model type\n",
    "\n",
    "- `Grid_search`\n",
    "    - Performs GridSearchCV for hyperparameter tuning depending on input model\n",
    "\n",
    "- `Get_feature_importance`\n",
    "    - Prints the top 8 and bottom 5 features\n",
    "\n",
    "- `Optimise_threshold`\n",
    "    - Optimises threshold based on Precision-Recall\n",
    "    - Our aim is to increase Precision-Recall (more importantly precision), since we want to increase the likelihood of winning a 1+ corners at 80min bet -> prediciting the number of 1's correctly as important, i.e. when we do place a bet, we make sure we have a high chance of winning.\n",
    "    - Therefore, I performed threshold adjustament to try and maximise precision (but ensure recall is at least 10% to avoid precision=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_model(model_name, hyperparameters):\n",
    "    if model_name == \"random_forest\":\n",
    "        return RandomForestClassifier(**hyperparameters, random_state=42, class_weight=\"balanced\")\n",
    "    elif model_name == \"logistic_regression\":\n",
    "        return LogisticRegression(**hyperparameters, random_state=42, class_weight=\"balanced\")\n",
    "    elif model_name == \"svc\":\n",
    "        return SVC(probability=True, **hyperparameters, random_state=42, class_weight=\"balanced\")\n",
    "    elif model_name == \"xgboost\":\n",
    "        return xgb.XGBClassifier(**hyperparameters, random_state=42)\n",
    "    else:\n",
    "        raise ValueError(f\"Model, {model_name}, is not supported.\")\n",
    "    \n",
    "def grid_search(model_name, model, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Performs Grid Search for hyperparameter tuning depending on input model:\n",
    "    \"\"\"\n",
    "    if model_name == \"random_forest\":\n",
    "        param_grid = param_grid or {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [None, 10, 20],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2,4],\n",
    "            'bootstrap': [True, False]\n",
    "        }\n",
    "\n",
    "    elif model_name == \"logistic_regression\":\n",
    "        param_grid = param_grid or {\n",
    "            'C': [0.1, 1, 10,100],\n",
    "            'solver': ['liblinear', 'saga']\n",
    "        }\n",
    "\n",
    "    elif model_name == \"svc\":\n",
    "        param_grid = param_grid or {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'kernel': ['linear', 'rbf'],\n",
    "            'gamma':['scale', 'auto']\n",
    "        }\n",
    "\n",
    "    elif model_name == \"xgboost\":\n",
    "        param_grid = param_grid or {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'learning_rate': [0.01, 0.1, 0.3],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'subsample': [0.8,1.0]\n",
    "        }\n",
    "\n",
    "    #Score by precision:\n",
    "    precision_scorer = make_scorer(precision_score, average=\"micro\")\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring=precision_scorer, n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best Parameters:\", grid_search.best_params_)\n",
    "    print(\"Best Precision Score:\", grid_search.best_score_)\n",
    "\n",
    "    #Use best_estimator for predictions\n",
    "    best_model = grid_search.best_estimator_\n",
    "    return best_model\n",
    "\n",
    "def get_feature_importance(model, model_name, selected_features, constructed_features):\n",
    "    \"\"\"\n",
    "    Gets feature importance (depending on model) \n",
    "    Displays top 8 + bottom 5...\n",
    "    \"\"\"\n",
    "    # Collect all features\n",
    "    all_features = selected_features + constructed_features\n",
    "    \n",
    "    if model_name in [\"random_forest\", \"xgboost\"]:\n",
    "        importance = model.feature_importances_\n",
    "    elif model_name == \"logistic_regression\":\n",
    "        importance = model.coef_[0]\n",
    "    elif model_name == \"svc\":\n",
    "        #For SVC, feature importance is only available for linear\n",
    "        if hasattr(model, 'coef_'):\n",
    "            importance = model.coef_[0]\n",
    "        else:  #Else, if its non-linear... set all importance to zero:\n",
    "            importance = np.zeros(len(all_features)) \n",
    "    else:\n",
    "        raise ValueError(f\"Invalid model input: {model_name}\")\n",
    "    \n",
    "    # Display feature importance\n",
    "    feature_importances = pd.DataFrame({\n",
    "        'Feature': all_features,\n",
    "        'Importance': importance\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    top_features = feature_importances.head(8) # Get top 8\n",
    "    bottom_features = feature_importances.tail(5) # Get bottom 5\n",
    "    \n",
    "    # Combine the top 8 and bottom 5 features\n",
    "    combined_features = pd.concat([top_features, bottom_features])\n",
    "\n",
    "    return combined_features\n",
    "\n",
    "def optimise_threshold(y_pred_val, y_val):\n",
    "    #--- Threshold Maximisation ---\n",
    "    thresholds = np.linspace(0.5, 0.95, 20) #test thresholds from 0.5 to 0.95\n",
    "    results = []\n",
    "    for t in thresholds:\n",
    "        y_pred_t =(y_pred_val>=t).astype(int)\n",
    "        precision_t= precision_score(y_val, y_pred_t, zero_division=1)  \n",
    "        recall_t= recall_score(y_val, y_pred_t)\n",
    "        \n",
    "        if recall_t>=0.1: #only take results where recall >= 0.1\n",
    "            results.append((t, precision_t, recall_t))\n",
    "\n",
    "    #results as table\n",
    "    print(\"\\n### Precision-Recall Tradeoff at Different Thresholds ###\\n\") \n",
    "    print(f\"{'Threshold':<12}{'Precision':<12}{'Recall':<12}\") \n",
    "    print(\"-\"*36)  \n",
    "    for t, p, r in results:\n",
    "        print(f\"{t:<12.2f}{p:<12.4f}{r:<12.4f}\")\n",
    "    \n",
    "    best_threshold = max(results, key=lambda x: x[1])\n",
    "    optimal_threshold = round(best_threshold[0],3)\n",
    "    print(\"\\n### Recommended Threshold for Maximum Precision ###\")\n",
    "    print(f\"Optimal Threshold: {optimal_threshold:.2f}\")\n",
    "    print(f\"Expected Precision: {best_threshold[1]:.4f}\")\n",
    "    print(f\"Expected Recall: {best_threshold[2]:.4f}\")\n",
    "\n",
    "    return best_threshold, optimal_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Utils \n",
    "\n",
    "- `Plot_scatter` - Plot scatter graph to visualise our spread of predictions & how the threshold binarises them. \n",
    "\n",
    "- `Plot_roc_and_prc` (calls `plot_roc_curve` and `plot_precision_recall_curve`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter(y_probs_final, y_test_final, optimal_threshold, model_name):\n",
    "    # Scatter plot: target vs predicted probabilities\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.scatter(range(len(y_probs_final)), y_probs_final,c=y_test_final,cmap='coolwarm',alpha=0.7,label='Predicted Probability')\n",
    "    plt.axhline(optimal_threshold, color='red', linestyle='dashed',linewidth=2,label=f'Threshold = {optimal_threshold:.2f}')\n",
    "    class_0 =mlines.Line2D([], [],color='blue',marker='o',linestyle='None', markersize=8, alpha=0.7, label='Target: No 1+ Corners')\n",
    "    class_1=mlines.Line2D([], [],color='red', marker='o',linestyle='None',markersize=8,alpha=0.7, label='Target: 1+ Corners')\n",
    "    plt.xlabel('Match Index')\n",
    "    plt.ylabel('Predicted probability of 1+ corners')\n",
    "    plt.title(f'{model_name} - Predicted Probability vs Actual Target')\n",
    "    plt.legend(handles=[class_0, class_1,plt.Line2D([], [],color='grey',linestyle='dashed',linewidth=2,label='Threshold')])\n",
    "\n",
    "    # Save graph as an image\n",
    "    image_path = f\"../reports/images/{model_name.replace(' ', '_').lower()}_scatter.png\"\n",
    "    plt.savefig(image_path)\n",
    "    # plt.close()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return image_path #return path for use later in markdwon func\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, roc_auc, model_name, ax):\n",
    "    ax.plot(fpr,tpr, color='b',lw=2, label=f'ROC curve (area ={roc_auc:.2f})')\n",
    "    ax.plot([0,1], [0,1], color='gray', linestyle='--')\n",
    "    ax.set_xlim([0.0,1.0])\n",
    "    ax.set_ylim([0.0,1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title(f'ROC Curve - {model_name}')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    \n",
    "def plot_precision_recall_curve(precision, recall,pr_auc, model_name, ax):\n",
    "    ax.plot(recall, precision, color='b', lw=2, label=f'Precision-Recall curve (AUC = {pr_auc:.2f})')\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_title(f'Precision-Recall Curve - {model_name}')\n",
    "    ax.legend(loc=\"lower left\")\n",
    "\n",
    "def plot_roc_and_prc(fpr, tpr, roc_auc, precision,recall, pr_auc,model_name):\n",
    "    #Plots ROC curve and Precision-recall side by side\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))  \n",
    "    plot_roc_curve(fpr, tpr, roc_auc, model_name,ax1)\n",
    "    plot_precision_recall_curve(precision, recall, pr_auc, model_name,ax2)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save graph as an image\n",
    "    image_path = f\"../reports/images/{model_name.replace(' ', '_').lower()}_roc_prc.png\"\n",
    "    plt.savefig(image_path)\n",
    "    # plt.close()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    return image_path\n",
    "\n",
    "def plot_classification_report(optimal_model, X_val, y_val, X_test, y_test, model_name):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    #Plot for validation set:\n",
    "    ConfusionMatrixDisplay.from_estimator(optimal_model, X_val, y_val,ax=axes[0],cmap='Blues',values_format='d')\n",
    "    axes[0].set_title(f'{model_name} - Validation Set')\n",
    "\n",
    "    #Plot for test set:\n",
    "    ConfusionMatrixDisplay.from_estimator(optimal_model, X_test, y_test,ax=axes[1],cmap='Blues',values_format='d')\n",
    "    axes[1].set_title(f'{model_name} - Test Set')\n",
    "\n",
    "    plt.subplots_adjust(left=0.05, right=0.95, top=0.9, bottom=0.1, wspace=0.3) #line reduces plot whitespace borders\n",
    "    # Save graph as an image\n",
    "    image_path =f\"../reports/images/{model_name.replace(' ', '_').lower()}_confusion_matrix.png\"\n",
    "    plt.savefig(image_path)\n",
    "    # plt.close()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    return image_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Pipeline\n",
    "\n",
    "- **Feature Selection**: \n",
    "    - Trains only on selected and constructed features\n",
    "- **Data Splitting**:\n",
    "    - Exclude the last 500 rows for testing\n",
    "    - Split remaining data into 80% train and 20% validation sets\n",
    "- **Model Training Loop**:\n",
    "    1) Initialise each model from config yaml\n",
    "    2) Apply MinMax scaling... *only for models that require scaling*\n",
    "    3) Perform grid search for hyperparameter tuning (if specified) and Train Model\n",
    "    4) Predict on validation set and display feature importance\n",
    "    5) Optimise precision-recall threshold\n",
    "    6) Evaluate model on validation set using optimised threshold\n",
    "    7) Plot ROC and Precision-Recall graphs\n",
    "    8) Predict on the test set (last 500 rows) and evaluate\n",
    "    9) Save the trained model\n",
    "    10) Save predictions (for backtesting).\n",
    "    11) Plot Scatter Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classification_pipeline(config, df, selected_features, constructed_features, target_variable=\"target\", show_output=False, generate_pdf=False): \n",
    "    models_to_train = config['model']['classification']['models']\n",
    "    \n",
    "    # Step 1: Create the Markdown for the report\n",
    "    markdown_content = create_markdown_report(config, feature_correlation_image_path, target_variable, selected_features, constructed_features, models_to_train)   \n",
    "    #Only train on selected and constructed features\n",
    "    X = df[selected_features + constructed_features]\n",
    "    y = df[target_variable]\n",
    "\n",
    "    #Split data to exclude the last 500 rows for testing\n",
    "    train_data = df.iloc[:-500]\n",
    "    test_data = df.iloc[-500:]\n",
    "\n",
    "    # Split data -> train & validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        train_data[selected_features + constructed_features],\n",
    "        train_data[\"target\"],\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=train_data[\"target\"]\n",
    "    )\n",
    "\n",
    "    # Create X and y for testset\n",
    "    X_test= test_data[selected_features+constructed_features]\n",
    "    y_test =test_data[\"target\"]\n",
    "\n",
    "    # MinMax Scaling for models that require scaling...\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled =scaler.transform(X_val)\n",
    "    X_test_scaled =scaler.transform(X_test)\n",
    "\n",
    "    # --- MODEL TRAINING LOOP ---\n",
    "    # Train and evaluate each model from the config\n",
    "    models_to_train = config['model']['classification']['models']\n",
    "    for model_name in models_to_train:\n",
    "        print(f\"\\n-> Training {model_name}...\")\n",
    "    \n",
    "        # Get the hyperparameters for the model\n",
    "        hyperparameters = config[\"model\"][\"classification\"][\"hyperparameters\"].get(model_name, {})\n",
    "        do_grid_search=config[\"model\"][\"classification\"].get(\"grid_search\", False)\n",
    "\n",
    "        # --- STEP 1: Initialise the model\n",
    "        model = initialise_model(model_name, hyperparameters)\n",
    "\n",
    "        # --- STEP 2: Select Scaled or Unscaled Data ---\n",
    "        if model_name in [\"logistic_regression\", \"svc\", \"xgboost\"]:\n",
    "            X_train, X_val, X_test = X_train_scaled, X_val_scaled, X_test_scaled\n",
    "        \n",
    "        # --- STEP 3: Grid Search (optional) ---\n",
    "        if do_grid_search:\n",
    "            optimal_model = grid_search(model_name, model, X_train, y_train)\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "            optimal_model = model\n",
    "\n",
    "        # --- STEP 4: Predict on the validation set ---\n",
    "        y_pred_val = optimal_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        #Display Feature Importance\n",
    "        feature_importances = get_feature_importance(optimal_model, model_name, selected_features, constructed_features)\n",
    "        print(\"\\n### Top 8 and Bottom 5 Feature Importance ###\")\n",
    "        print(feature_importances)\n",
    "\n",
    "        # --- STEP 5: Precision-Recall Threshold Optimisation ---\n",
    "        best_threshold, optimal_threshold = optimise_threshold(y_pred_val, y_val)\n",
    "\n",
    "        # --- STEP 6: Model Evaluation ---\n",
    "        y_pred_threshold =(y_pred_val >= optimal_threshold).astype(int)\n",
    "        print(f\"\\n### Classification Report (threshold={optimal_threshold}):\\n\") \n",
    "        print(classification_report(y_val, y_pred_threshold))\n",
    "        classification_report_str_1 = classification_report(y_val, y_pred_threshold, output_dict=False)\n",
    "\n",
    "        # --- STEP 7: Plot ROC and Precision-Recall curves ---\n",
    "        fpr,tpr,_ =roc_curve(y_val, y_pred_val)\n",
    "        roc_auc =roc_auc_score(y_val, y_pred_val)\n",
    "        precision,recall,_ =precision_recall_curve(y_val, y_pred_val)\n",
    "        pr_auc=auc(recall, precision)\n",
    "\n",
    "        # --- STEP 8: Test on test set (last 500 rows) ---\n",
    "        # Predict on the final test data (last 500 rows)\n",
    "        y_probs_final =optimal_model.predict_proba(X_test)[:, 1]\n",
    "        y_pred_final=(y_probs_final >=optimal_threshold).astype(int)\n",
    "\n",
    "        #Evaluate on the last 500 rows (final simulation)\n",
    "        print(\"\\n### Prediction on last 500 rows: ###\")\n",
    "        print(classification_report(y_test, y_pred_final))\n",
    "        classification_report_str_2 = classification_report(y_test, y_pred_final, output_dict=False)\n",
    "\n",
    "        classification_report_image_path = plot_classification_report(optimal_model, X_val,y_val,X_test,y_test, model_name)\n",
    "        roc_prc_image_path = plot_roc_and_prc(fpr, tpr, roc_auc,precision,recall, pr_auc,model_name)\n",
    "\n",
    "        # --- STEP 9: Save Model ---\n",
    "        model_dir = \"../models\" \n",
    "        if not os.path.exists(model_dir): # Ensure the directory exists\n",
    "            os.makedirs(model_dir)\n",
    "        # Save the model\n",
    "        joblib.dump(optimal_model, os.path.join(model_dir, f\"{model_name.replace(' ', '_').lower()}_model.pkl\"))\n",
    "        print(f\"{model_name} model saved.\")\n",
    "\n",
    "        # --- STEP 10: Save Predictions ---\n",
    "        results_df = pd.DataFrame({\n",
    "            'kaggle_id': test_data['id_odsp'],\n",
    "            'model_predicted_binary': y_pred_final,\n",
    "            'actual_result': y_test\n",
    "        })\n",
    "        results_df.to_csv(f\"../data/predictions/{model_name.replace(' ', '_').lower()}_predictions.csv\", index=False)\n",
    "        print(f\"Predictions saved for {model_name}.\")\n",
    "\n",
    "        # --- STEP 11: Plot Scatter Graph ---\n",
    "        scatter_image_path = plot_scatter(y_probs_final, y_test, optimal_threshold, model_name)\n",
    "\n",
    "        #Finally, update markdown with generated outputs...\n",
    "        markdown_content = update_markdown_with_model_details(\n",
    "            markdown_content,\n",
    "            model_name,\n",
    "            feature_importances,\n",
    "            best_threshold,\n",
    "            classification_report_str_1,\n",
    "            classification_report_str_2,\n",
    "            classification_report_image_path,\n",
    "            roc_prc_image_path,\n",
    "            scatter_image_path\n",
    "        )\n",
    "    \n",
    "    now = datetime.now()\n",
    "    date_time_str = now.strftime(\"%Y-%m-%d, %H:%M\")\n",
    "    # Convert Markdown to html and save as pdf to reports/model_reports/\n",
    "    html_content = convert_markdown_to_html(markdown_content)\n",
    "    save_pdf_from_html(html_content, f'../reports/model_reports/{date_time_str}_model_report.pdf')\n",
    "\n",
    "''' \n",
    "*****----------------------------------------*****\n",
    "MAIN training and testing loop for classification\n",
    "*****----------------------------------------*****\n",
    " '''\n",
    "run_classification_pipeline(config, df_selected, selected_features, constructed_features, show_output=False, generate_pdf=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
