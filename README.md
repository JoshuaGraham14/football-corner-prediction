# football-corner-prediction

This project aims to predict late-game football corner kicks using machine learning models, evaluating them using backtesting. The motivation is based on the hypothesis that bookmaker odds are undervalued, especially when a team is trailing by a small margin late in a match.

The system is divided into two distinct prediction tracks: 
- **Track 1**: Looking at all matches, the model predicts whether _any team_ will earn at least one corner after the 80th minute.
- **Track 2**: Focusing only on matches where a team is trailing by exactly one goal at the 80th minute, the model predicts whether _the trailing team_ will earn at least one corner.

## Project Structure

The project is designed and built to be run through Jupyter notebooks, with the `notebooks` directory containing all code related to the machine learning pipeline. It is split into actual Python notebooks (for running the pipeline) and several helper modules (to abstract and provide reusable, modular functions for different aspects of the pipeline, such as model training, backtesting, and result reporting). These helper modules are situated inside the `notebooks` directory to simplify importing (since it allows relative imports).

### Notebooks
The `notebooks` directory contains all code related to the machine learning pipeline. The notebooks follow a numbered naming convention, as described below:

1. **Data Collection and Feature Engineering (`01.xx.ipynb`)**: Includes scripts for data aggregation, cleaning, and feature construction, including constructing specific features for each Track's distinct dataset.
2. **Exploratory Data Analysis (EDA) (`02.xx.ipynb`)**: Contains notebooks for analysing datasets, visualising trends, and understanding feature correlations.
3. **Classification/Prediction Pipelines (`03.xx.ipynb`)**: Implements machine learning pipelines for both tracks, including model training, evaluation, and backtesting.

### Notebook Modules
The `notebooks` directory also includes several modular Python scripts that provide reusable functions for the pipeline:

1. **Utilities** (`notebooks/utils`): directory contains helper functions and modules for model training, PDFs, and plotting visualizations.
    - **`model_utils.py`**: Includes functions for model initialisation, hyperparameter tuning (grid search), feature importance extraction, and threshold optimisation.
    - **`plotting_utils.py`**: Functions for generating plots, including dataset splitting, feature correlation heatmaps, ROC & precision-recall curves, and prediction distribution graphs.
    - **`pdf_utils.py`**: Functions for generating Markdown-based PDF reports, including pipeline configuration, model training plots, and backtesting results.
    - **`config_utils.py`**: Responsible for loading a specified configuration file.
    - **`data_utils.py`**: Responsible for loading datasets and extracting the specified features.
    - **`classification_pipeline.py`**: Implements the end-to-end pipeline for training, evaluating, and backtesting, for a specified config file (which specifies Track 1 or Track 2).

2. **Backtesting** (`notebooks/backtesting`): Class for simulating a backtesting system to evaluates model performance.
    - **`simulator.py`**: Base class for simulating and tracking each bet placed strategies, bankroll management and performance metrics (ROI, profit, etc.).
    - **`backtester.py`**: Extends the simulator to run backtesting, looking at predictions, choosing whether to place bets, and generating bankroll growth figures.
  
3. **Model Registry**: Tracks and stores pipeline results for each model trained, including features selected, models trained, and evaluative metrics.

### Data
The `data` directory contains all datasets used in the project, split into:
- **Raw Kaggle**: Unprocessed data from the Kaggle _Football Events_ dataset.
- **Processed Data**: Cleaned and feature-engineered datasets.
- **TotalCorner**: Datasets fetched from TotalCorner API.
- **Predictions**: Predictions generated from each model (used for backtesting)

### Reports
The `reports` directory stores generated outputs from pipeline runs, including:
- **Figures**: Visualisaations and plots from analysis and model evaluation.
- **Model Reports**: PDF reports summarising model performance and backtesting results. It also contains the optimal results found, which are presented in the report.
- **Model Registry Results**: CSVs of results of all trained models across unique pipelines.

### Models
The `models` directory contains the trained machine learning models generated by the pipeline (as `.pkl` files), organised by Track.
**Note**: While the saved models are not directly used in this project, they are stored as a convention and for potential future use and further analysis.

### Scripts
The `scripts` directory contains utility scripts for interacting with the TotalCorner API, including odds retrieval. To use these scripts, an API key is required, which should be added to the `.env` file.

## How to Use/Run the System

1. Uncomment or comment the selected features in the configuration files (`config_track1.yaml` or `config_track2.yaml`).
2. Open the corresponding notebook (`track_1_run_pipeline.ipynb` or `track_2_run_pipeline.ipynb`).
3. Restart the notebook kernel.
4. Run all cells in the notebook.

**Important**: If changes are made to the configuration file, the kernel must be reset before re-running the notebook in order to sync the changes.

You can also run other notebooks in the notebooks directory for specific tasks, such as Exploratory Data Analysis (EDA).

## Installation Instructions

Install the required dependencies using `pip`:

```bash
pip install -r requirements.txt
